{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigray ML Platform on SageMaker\n",
    "\n",
    "This notebook demonstrates how to use the Sigray Machine Learning Platform on AWS SageMaker for 3D image enhancement.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Setting up the environment\n",
    "2. Preparing training data\n",
    "3. Running a training job\n",
    "4. Deploying for inference\n",
    "5. Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Sigray ML Platform\n",
    "!git clone https://github.com/tianzhuqin-argonne/Image_quality_enhancement.git\n",
    "%cd Image_quality_enhancement\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sagemaker\n",
    "import boto3\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Import Sigray ML Platform\n",
    "from src.inference.api import ImageEnhancementAPI\n",
    "from src.core.config import InferenceConfig\n",
    "from src.testing.test_fixtures import TestDataFixtures\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Upload Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic training data\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Creating training data in: {temp_dir}\")\n",
    "\n",
    "with TestDataFixtures(temp_dir) as fixtures:\n",
    "    # Create training dataset\n",
    "    dataset = fixtures.create_test_dataset(\n",
    "        num_pairs=10,\n",
    "        volume_size=\"small\",\n",
    "        degradation_level=\"moderate\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(dataset)} training pairs\")\n",
    "    \n",
    "    # Organize data for SageMaker\n",
    "    training_data_dir = Path(temp_dir) / \"training_data\"\n",
    "    input_dir = training_data_dir / \"input\"\n",
    "    target_dir = training_data_dir / \"target\"\n",
    "    \n",
    "    input_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save training pairs\n",
    "    for i, (input_vol, target_vol) in enumerate(dataset):\n",
    "        input_path = fixtures.save_test_tiff(input_vol, f\"input_{i:03d}\")\n",
    "        target_path = fixtures.save_test_tiff(target_vol, f\"target_{i:03d}\")\n",
    "        \n",
    "        # Move to organized structure\n",
    "        shutil.move(input_path, input_dir / f\"input_{i:03d}.tif\")\n",
    "        shutil.move(target_path, target_dir / f\"target_{i:03d}.tif\")\n",
    "    \n",
    "    print(f\"Training data organized in: {training_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data to S3\n",
    "training_s3_path = f\"s3://{bucket}/sigray-ml-training-data\"\n",
    "\n",
    "training_input = sagemaker_session.upload_data(\n",
    "    path=str(training_data_dir),\n",
    "    bucket=bucket,\n",
    "    key_prefix=\"sigray-ml-training-data\"\n",
    ")\n",
    "\n",
    "print(f\"Training data uploaded to: {training_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'epochs': 10,  # Reduced for demo\n",
    "    'batch-size': 4,\n",
    "    'learning-rate': 1e-4,\n",
    "    'patch-size': '128,128',  # Smaller for demo\n",
    "    'overlap': 16,\n",
    "    'optimizer': 'adam',\n",
    "    'loss-function': 'mse',\n",
    "    'device': 'auto',\n",
    "    'early-stopping-patience': 5,\n",
    "    'use-augmentation': True\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point='sagemaker_train.py',\n",
    "    source_dir='sagemaker',\n",
    "    role=role,\n",
    "    instance_type='ml.p3.2xlarge',  # GPU instance\n",
    "    instance_count=1,\n",
    "    volume_size=30,  # GB\n",
    "    max_run=3600,    # 1 hour max\n",
    "    framework_version='1.12.0',\n",
    "    py_version='py38',\n",
    "    hyperparameters=hyperparameters,\n",
    "    environment={\n",
    "        'PYTHONPATH': '/opt/ml/code'\n",
    "    },\n",
    "    # Enable spot instances for cost savings (optional)\n",
    "    use_spot_instances=True,\n",
    "    max_wait=7200,\n",
    "    checkpoint_s3_uri=f's3://{bucket}/sigray-ml-checkpoints/'\n",
    ")\n",
    "\n",
    "print(\"âœ… Estimator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training job\n",
    "print(\"ðŸš€ Starting training job...\")\n",
    "\n",
    "estimator.fit({\n",
    "    'training': training_input\n",
    "})\n",
    "\n",
    "print(\"âœ… Training job completed!\")\n",
    "print(f\"Model artifacts: {estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Model for Real-time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch model from training job\n",
    "model = PyTorchModel(\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,\n",
    "    entry_point='sagemaker_inference.py',\n",
    "    source_dir='sagemaker',\n",
    "    framework_version='1.12.0',\n",
    "    py_version='py38'\n",
    ")\n",
    "\n",
    "print(\"âœ… Model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model as endpoint\n",
    "print(\"ðŸš€ Deploying model endpoint...\")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    endpoint_name='sigray-ml-endpoint'\n",
    ")\n",
    "\n",
    "print(\"âœ… Endpoint deployed!\")\n",
    "print(f\"Endpoint name: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Real-time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "test_image = np.random.rand(3, 128, 128).astype(np.float32)\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "\n",
    "# Prepare input for endpoint\n",
    "import base64\n",
    "\n",
    "# Serialize numpy array\n",
    "image_bytes = test_image.tobytes()\n",
    "image_b64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Create request payload\n",
    "payload = {\n",
    "    'image_data': image_b64,\n",
    "    'shape': test_image.shape,\n",
    "    'metadata': {\n",
    "        'source': 'notebook_test',\n",
    "        'timestamp': '2025-01-14'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Test payload prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "print(\"ðŸ”® Making prediction...\")\n",
    "\n",
    "# Set content type and accept headers\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.accept = 'application/json'\n",
    "\n",
    "# Make prediction\n",
    "result = predictor.predict(payload)\n",
    "\n",
    "print(\"âœ… Prediction completed!\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Processing time: {result.get('processing_time', 'N/A')} seconds\")\n",
    "\n",
    "if result['success']:\n",
    "    enhanced_shape = np.array(result['enhanced_array']).shape\n",
    "    print(f\"Enhanced image shape: {enhanced_shape}\")\n",
    "    \n",
    "    if 'quality_metrics' in result:\n",
    "        metrics = result['quality_metrics']\n",
    "        print(\"Quality metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            if key != 'enhanced_array':  # Skip the large array\n",
    "                print(f\"  {key}: {value}\")\nelse:\n",
    "    print(f\"Error: {result.get('error_message', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch input data\n",
    "batch_input_dir = Path(temp_dir) / \"batch_input\"\n",
    "batch_input_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create several test images\n",
    "for i in range(3):\n",
    "    test_vol = fixtures.get_small_test_volume()\n",
    "    test_path = fixtures.save_test_tiff(test_vol, f\"batch_test_{i}\")\n",
    "    shutil.move(test_path, batch_input_dir / f\"test_{i}.tif\")\n",
    "\n",
    "print(f\"Created batch input data in: {batch_input_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload batch input to S3\n",
    "batch_input_s3 = sagemaker_session.upload_data(\n",
    "    path=str(batch_input_dir),\n",
    "    bucket=bucket,\n",
    "    key_prefix=\"sigray-ml-batch-input\"\n",
    ")\n",
    "\n",
    "print(f\"Batch input uploaded to: {batch_input_s3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch transformer\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    output_path=f's3://{bucket}/sigray-ml-batch-output/',\n",
    "    accept='application/json'\n",
    ")\n",
    "\n",
    "print(\"âœ… Transformer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start batch transform job\n",
    "print(\"ðŸš€ Starting batch transform job...\")\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_input_s3,\n",
    "    content_type='image/tiff',\n",
    "    split_type='None'\n",
    ")\n",
    "\n",
    "print(\"âœ… Batch transform completed!\")\n",
    "print(f\"Output location: {transformer.output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job metrics\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "# Get training job name\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "\n",
    "# Get CPU utilization metrics\n",
    "try:\n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='AWS/SageMaker',\n",
    "        MetricName='CPUUtilization',\n",
    "        Dimensions=[\n",
    "            {\n",
    "                'Name': 'TrainingJobName',\n",
    "                'Value': training_job_name\n",
    "            }\n",
    "        ],\n",
    "        StartTime=datetime.utcnow() - timedelta(hours=2),\n",
    "        EndTime=datetime.utcnow(),\n",
    "        Period=300,\n",
    "        Statistics=['Average']\n",
    "    )\n",
    "    \n",
    "    if response['Datapoints']:\n",
    "        avg_cpu = sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])\n",
    "        print(f\"Average CPU utilization: {avg_cpu:.2f}%\")\n",
    "    else:\n",
    "        print(\"No CPU metrics available yet\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and examine training summary\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Parse model data S3 path\n",
    "model_s3_path = estimator.model_data\n",
    "bucket_name = model_s3_path.split('/')[2]\n",
    "key_prefix = '/'.join(model_s3_path.split('/')[3:-1])\n",
    "\n",
    "print(f\"Model artifacts location: s3://{bucket_name}/{key_prefix}/\")\n",
    "\n",
    "# List available artifacts\n",
    "try:\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=key_prefix\n",
    "    )\n",
    "    \n",
    "    print(\"Available artifacts:\")\n",
    "    for obj in response.get('Contents', []):\n",
    "        print(f\"  {obj['Key']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not list artifacts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint to avoid charges\n",
    "print(\"ðŸ§¹ Cleaning up resources...\")\n",
    "\n",
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    print(\"âœ… Endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "# Clean up local temporary files\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(\"âœ… Temporary files cleaned up\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning up temp files: {e}\")\n",
    "\n",
    "print(\"ðŸŽ‰ Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. âœ… **Environment Setup** - Installed Sigray ML Platform on SageMaker\n",
    "2. âœ… **Data Preparation** - Created and uploaded training data to S3\n",
    "3. âœ… **Training Job** - Ran distributed training on GPU instances\n",
    "4. âœ… **Real-time Inference** - Deployed model as SageMaker endpoint\n",
    "5. âœ… **Batch Processing** - Processed multiple images with batch transform\n",
    "6. âœ… **Monitoring** - Analyzed training metrics and performance\n",
    "7. âœ… **Cleanup** - Removed resources to avoid charges\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Scale up with larger datasets and longer training\n",
    "- Experiment with different hyperparameters\n",
    "- Set up automated pipelines with SageMaker Pipelines\n",
    "- Implement A/B testing for model versions\n",
    "- Add custom metrics and monitoring\n",
    "\n",
    "### Cost Optimization Tips\n",
    "\n",
    "- Use spot instances for training (up to 90% savings)\n",
    "- Choose appropriate instance types for your workload\n",
    "- Delete endpoints when not in use\n",
    "- Use batch transform for large-scale inference\n",
    "- Monitor usage with AWS Cost Explorer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}